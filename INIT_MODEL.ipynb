{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9db5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_name =\"jhu-clsp/mmBERT-base\"\n",
    "#model_name = \"deepvk/USER2-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_orig = AutoTokenizer.from_pretrained(model_name)\n",
    "model_orig = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f7366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from improved_collator import ImprovedUL2Collator\n",
    "\n",
    "collator = ImprovedUL2Collator(\n",
    "        tokenizer=tokenizer_orig,\n",
    "        max_input_length=512,\n",
    "        max_target_length=512,\n",
    "        ul2_denoiser_probs=[0.5, 0.25, 0.25],\n",
    "        r_denoiser_suffix_ratio=0.25,\n",
    "        s_denoiser_corrupt_prob=0.15,\n",
    "        x_denoiser_corrupt_prob=0.5,\n",
    "        mean_span_length=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize model embeddings to match tokenizer\n",
    "model_orig.resize_token_embeddings(len(collator.tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93675a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"./final_pretrain_mix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313aeecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = load_from_disk(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВАЖНО! ПРОВЕРЬТЕ СУЩЕСТВУЮТ ЛИ У МОДЕЛЕЙ токены EOS и т.д\n",
    "-tokenizer_orig\n",
    "\n",
    "# Clean configuration\n",
    "config_keys_to_update = {\n",
    "    'decoder_start_token_id': tokenizer_orig.bos_token_id or tokenizer_orig.cls_token_id or 0,\n",
    "    'pad_token_id': tokenizer_orig.pad_token_id or 0,\n",
    "    'bos_token_id': tokenizer_orig.bos_token_id or tokenizer_orig.cls_token_id,\n",
    "    'eos_token_id': tokenizer_orig.eos_token_id or tokenizer_orig.sep_token_id,\n",
    "    'tie_word_embeddings': True,\n",
    "    'is_encoder_decoder': True,\n",
    "    'num_decoder_layers': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f993db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from fixed_modernt5 import ModernBertModel, ModernT5Config, ModernT5ForConditionalGeneration\n",
    "\n",
    "# Get the actual encoder model (handle different architectures)\n",
    "if hasattr(model_orig, 'bert'):\n",
    "    encoder_model = model_orig.bert\n",
    "elif hasattr(model_orig, 'roberta'):\n",
    "    encoder_model = model_orig.roberta\n",
    "elif hasattr(model_orig, 'model'):\n",
    "    encoder_model = model_orig.model\n",
    "else:\n",
    "    raise ValueError(f\"Cannot find encoder in model of type {type(model_orig)}\")\n",
    "\n",
    "# Configure ModernT5\n",
    "encoder_config_dict = model_orig.config.to_dict()\n",
    "\n",
    "encoder_config_dict.update(config_keys_to_update)\n",
    "config = ModernT5Config(**encoder_config_dict)\n",
    "\n",
    "# Initialize model\n",
    "model = ModernT5ForConditionalGeneration(config)\n",
    "model.model.encoder = encoder_model\n",
    "\n",
    "print(\"Tying word embeddings between encoder, decoder, and LM head...\")\n",
    "# Ensure the embeddings are shared across the new encoder, the decoder, and the LM head.\n",
    "# We get the embeddings from the new encoder and set them for the whole model.\n",
    "model.set_input_embeddings(model.get_encoder().get_input_embeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Steps (Verification, Saving)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Model moved to {device.upper()}\")\n",
    "\n",
    "# Print parameter count\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_params / 1e6:.2f} M\")\n",
    "\n",
    "# Save the composed model and the tokenizer\n",
    "save_directory = \"./modernt5_from_mmBERT-base_e21_d1\"\n",
    "print(f\"Saving model and tokenizer to {save_directory}...\")\n",
    "model.save_pretrained(save_directory)\n",
    "collator.tokenizer.save_pretrained(save_directory)\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2301aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Load and Test ---\n",
    "print(f\"\\nLoading model from {save_directory}...\")\n",
    "loaded_model = ModernT5ForConditionalGeneration.from_pretrained(save_directory)\n",
    "loaded_model.to(device)\n",
    "print(\"Model loaded successfully from checkpoint.\")\n",
    "\n",
    "# Test with a dummy forward pass\n",
    "print(\"Performing a dummy forward pass...\")\n",
    "dummy_input_ids = torch.randint(0, config.vocab_size, (2, 16)).to(device)\n",
    "dummy_labels = torch.randint(0, config.vocab_size, (2, 10)).to(device)\n",
    "dummy_output = loaded_model(input_ids=dummy_input_ids, labels=dummy_labels)\n",
    "print(f\"Dummy forward pass successful. Loss: {dummy_output.loss.item():.4f}\")\n",
    "\n",
    "# Test generation\n",
    "print(\"\\nPerforming a dummy generation...\")\n",
    "# Use the tokenizer to prepare input\n",
    "prompt = \"Перевод с русского на английский: как дела?\"\n",
    "inputs = collator.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output\n",
    "generated_ids = loaded_model.generate(**inputs, max_length=50)\n",
    "decoded_text = collator.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: '{prompt}'\")\n",
    "print(f\"Generated output: '{decoded_text}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
