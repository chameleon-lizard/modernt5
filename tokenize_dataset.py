import os
import logging
import argparse
from datasets import load_from_disk
from transformers import AutoTokenizer, set_seed


def parse_args():
    parser = argparse.ArgumentParser(description="Tokenize a dataset using a Hugging Face tokenizer")
    parser.add_argument("--dataset_dir", type=str, default="final_pretrain_mix",
                        help="Path to the directory containing the raw dataset.")
    parser.add_argument("--tokenizer_path", type=str, default="modernt5_tokenizer",
                        help="Path to the directory containing the tokenizer.")
    parser.add_argument("--output_dir", type=str, default="final_pretrain_mix_tokenized",
                        help="Directory to save the tokenized dataset.")
    parser.add_argument("--max_seq_length", type=int, default=2048,
                        help="Maximum sequence length for tokenization.")
    parser.add_argument("--num_workers", type=int, default=os.cpu_count(),
                        help="Number of workers to use for tokenization.")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed.")
    return parser.parse_args()


def main():
    args = parse_args()

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger = logging.getLogger(__name__)

    # Set seed for reproducibility
    set_seed(args.seed)

    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)

    # Load dataset
    logger.info(f"Loading dataset from {args.dataset_dir}")
    # Assuming the dataset has a 'text' column based on typical raw datasets
    try:
        dataset = load_from_disk(args.dataset_dir)
        if "text" not in dataset.column_names:
             raise ValueError(f"Dataset must contain a 'text' column. Found: {dataset.column_names}")
    except Exception as e:
        logger.error(f"Failed to load dataset or 'text' column not found: {e}")
        return


    # Load tokenizer
    logger.info(f"Loading tokenizer from {args.tokenizer_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    except Exception as e:
        logger.error(f"Failed to load tokenizer from {args.tokenizer_path}: {e}")
        return

    # Tokenization function
    def tokenize_function(examples):
        # Use truncation=True and padding=False here. Padding will be done by the collator.
        # max_length will be used to truncate sentences longer than max_seq_length.
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.max_seq_length,
            padding=False, # Collator handles padding
        )

    # Tokenize the dataset
    logger.info(f"Tokenizing dataset with max_seq_length={args.max_seq_length} using {args.num_workers} workers...")
    # We need to remove the original 'text' column after tokenization
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=args.num_workers,
        remove_columns=["text"], # Remove the original text column
    )

    # Ensure the 'labels' column is not present yet, as the collator creates it
    # If it exists from a previous run or a different process, remove it.
    if 'labels' in tokenized_dataset.column_names:
        tokenized_dataset = tokenized_dataset.remove_columns(['labels'])
        logger.warning("Removed existing 'labels' column as it will be generated by the collator.")

    # Calculate total number of tokens
    logger.info("Calculating the total number of tokens in the dataset...")
    lengths = tokenized_dataset.map(lambda x: {'length': len(x['input_ids'])}, num_proc=args.num_workers)
    total_tokens = sum(lengths['length'])
    logger.info(f"The tokenized dataset contains {total_tokens:,} tokens.")

    # Save dataset info to a markdown file
    info_file_path = os.path.join(args.output_dir, "dataset_info.md")
    with open(info_file_path, "w") as f:
        f.write(f"# Dataset Info\n\n")
        f.write(f"Total number of tokens: {total_tokens:,}\n")
    logger.info(f"Dataset info saved to {info_file_path}")


    # Save the tokenized dataset
    logger.info(f"Saving tokenized dataset to {args.output_dir}")
    tokenized_dataset.save_to_disk(args.output_dir)

    logger.info("Tokenization and saving completed successfully.")


if __name__ == "__main__":
    main()
